\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
%\usepackage{mdframed}% for framed figures
%\colorlet{framecolor}{gray}

\usepackage{natbib} 
\usepackage{ulem}%for strike through, can be deleted when submitting the paper
%\usepackage[backend=bibtex, style=authoryear]{biblatex} %compile on this order --> LaTeX BiBTeX LaTeX
%\addbibresource{refs.bib}
\usepackage{amsmath,amsthm,amsfonts,dsfont}
\usepackage{mathrsfs}  
\usepackage{tikz}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{tabularx}
\usepackage[hidelinks]{hyperref}
\usepackage{float}
\usepackage{subfigure}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{property}[theorem]{Property}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}


\usepackage[inline]{enumitem}

\newcommand{\restrict}{\mid}
\newcommand{\range}[1]{\mathscr{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Greek
\newcommand{\zdistparama}{\alpha}
\newcommand{\zdistparamb}{\beta}
\newcommand{\zdistparamc}{\gamma}
\newcommand{\freeGamma}{\Gamma}
\newcommand{\order}{\delta}
\newcommand{\freeDelta}{\Delta}

\newcommand{\dominantU}{\nu}
\newcommand{\dominantUbar}{\bar{\nu}}
\newcommand{\sampledensity}{\mathbf{\pi}}
\newcommand{\intensity}{\lambda}
\newcommand{\Intensity}{\Lambda}
\newcommand{\dominantY}{\eta}
\newcommand{\dominantYbar}{\bar{\eta}}
\newcommand{\intensityratio}{\rho}
\newcommand{\densityratio}{\rho}
\newcommand{\parampop}{\theta}
\newcommand{\permutation}{\tau}
\newcommand{\paramnuisance}{\xi}
\newcommand{\provar}{\Sigma}
\newcommand{\Residual}{\varepsilon}
%\newcommand{\Covf}{\Gamma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Latin

\newcommand{\acos}{acos}
\newcommand{\subsetA}{A}
\newcommand{\Covariogram}{C}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\derive}{\mathrm{d}}
\newcommand{\Design}{D}
\newcommand{\design}{\mathbf{d}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\density}{\mathrm{f}}
\newcommand{\Semivariogram}{G}
\newcommand{\indicator}{I}
\newcommand{\Sampleindex}{K}
\newcommand{\sampleindex}{\mathbf{k}}
\newcommand{\likelihood}{\mathscr{L}}
%\newcommand{\popsize}{N}
\newcommand{\Samplesize}{N}
\newcommand{\samplesize}{\mathbf{n}}
%\newcommand{\P}{P}
\newcommand{\Sample}{S}
\newcommand{\sample}{\mathbf{s}}
\newcommand{\size}{\mathrm{size}}
\newcommand{\Pop}{\mathrm{U}}
\newcommand{\toPop}{\bar{\mathrm{U}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Position}{X}
\newcommand{\position}{\mathbf{x}}
\newcommand{\SignalSpace}{\mathscr{Y}}
\newcommand{\toSignalSpace}{\bar{\SignalSpace}}
\newcommand{\Signal}{Y}
\newcommand{\signal}{\mathbf{y}}
\newcommand{\Desvar}{Z}
\newcommand{\DesvarSpace}{\mathscr{Z}}
\newcommand{\desvar}{\mathbf{z}}

\newcommand{\xxx}{.5}
\newcommand{\yyy}{.5}
\newcommand{\hhh}{.5}
\newcommand{\xx}[1]{\renewcommand{\xxx}{#1}}
\newcommand{\hh}[1]{\renewcommand{\hhh}{#1}}
\newcommand{\yy}[1]{\renewcommand{\yyy}{#1}}

\newcommand\smallplot{
\begin{tikzpicture}
    \draw (0, 0) rectangle (1, 1);
    \draw[blue] (\xxx,\yyy+\hhh) arc(90:-90:\hhh);
  \end{tikzpicture}}

\title{Informative Selection and Spatial Process}
\date{}
\author{Bonn\'ery,  Pantalone, Ranalli}
\begin{document}
\normalem %delete this line when ulem packages is removed

\maketitle

\tableofcontents

\begin{abstract}
This paper extends the concept of informative selection, population distribution and sample distribution in a spatial process context.
Those notions were defined by 
\cite{pfefferman_1992} in a context where the output of the random process of interest consists of independent and identically distributed realisations for each individuals of a population. \cite{BonneryBreidtCoquet} showed that informative selection was inducing a stochastic dependence among realisations on the selected units. In the context of spatial process, the "population" is a continuous space and realisations for two different elements of the population are not independent. We show how informative selection may induce a different dependence among selected units, how the sample distribution differs from the "population" distribution, and how one can  account for this effect in an simulated study when doing statistical inference, including Semi-variogram parametric and semi parametric estimation, as well as prediction on the part of the space for which the random process was not observed.
\end{abstract}

{\color{red} The references in abstracts should be either written in full or not given}.

\section{Intro}

\input{section1}
\input{section2}
\input{section3}
\input{section4}
\input{section5}


\appendix
\input{sectionA.tex}
\section{R code}


We generate a spatial Gaussian random process by means of the package \texttt{RandomFields}.


\bibliographystyle{apalike}
\bibliography{refs}

\url{https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/nichols/research/spatbayes/johnson_spatialpointproc.pdf}

Kaar
Matheron.
Pfeffermann

\section{things deleted}

\sout{The sampling design could be very different according to the application, and could or not reflects some structure of the population. Every variable used in the selection is called \emph{design variable}. The \emph{probability proportional to size (pps)} sampling uses a variable $z$, or proxy variable, in order to select units  such that per each draw $k=1,...,n$, $P_{i}=P\left(I_{\position_{i}=1}\right)=z_{i}/\sum_{j=1}^{N}{z_{j}}$. The proxy variable has to be known for the entire population and usually it is correlated with the variable of interest. In this case the variable $z$ would be the design variable. For example, if interested in the level of specific pollutant, we could sample points with probability proportional to another variable correlated to it. In predicting areal rainfall, \cite{Kaar} considers that the points at which $Y$ is observed are the points $\position_{i}$ of a homogeneous two-dimensional Poisson process independent of $\Signal$. \emph{Spatially balanced sampling designs} select sample well spread over the area of interest, assigning low probabilities to be in the same sample to units close between them.}

{\color{red}
We need to add selected dots to the graph}




\appendix
\section{A kind of roadmap!}

    \paragraph{First step - simulation (done!)}
Illustrate through simulations the notion of sample variogram.\\
Setup:
\begin{itemize}    

    \item \textbf{simulation}: we use the dataset \texttt{meuse} as it were the population, compute the "real" variogram using all data, then sample by the selected sampling design and estimate the sample variogram;
    \item \textbf{code}: \url{https://github.com/DanielBonnery/Meuse}
    \item \textbf{results}: \url{https://github.com/DanielBonnery/Meuse/blob/master/demo/var_est.md}
\end{itemize}
\paragraph{Second step - theoretical development (to be done!)}
We need to model the spatial process $Y$ and $Z|Y$, with $Z$ spatial process of a design variable. From these, derive the distribution of 
\begin{itemize}
    \item $(I|Y,Z)$
    \item $(I|Z)$
\end{itemize}
then derive
\begin{itemize}
    \item $((Y(\textbf{x}_{1},Y\textbf{x}_{2})|I(\textbf{x}_{1})=I(\textbf{x}_{2})=1)$
\end{itemize}

The plan is:
\begin{enumerate}
    \item choose the covariance matrix for the spatial process $\Sigma(\theta,h)$, where $h$ is the distance;
    \item choose the design;
    \item express $\Sigma$ of sample;
    \item check that the estimated of the variogram bounces around theoretical sample variogram;
    \item derive results on the converge to sample variogram if possible.
\end{enumerate}



\section{Sketch}
The space $\Pop$ from which we sample from and a spatial process $Y:\Omega:\to \left(\Pop\to\range{Y}\right)$ with $\position\in\range{U}$, $\Cov\left(\Signal(\position_{1}), \Signal(\position_{2})\right)=\Semivariogram\left(\left|\position_{1}-\position_{2}\right|\right)$ and stationarity and isotropy assumption {\color{red}more on spatial model: dominated, fully parametric. Write density (likelihood of $Y$) with examples. Define f and g}

 $f(\position_1,\position_2,\signal_1,\signal_2)=\left(\mathrm{d}P^{\Signal(\position_1),\Signal(\position_2)}/\mathrm{d}\dominantY\right)(\signal_1,\signal_2)$
    
    we may limit ourselves to models such that exists $g$ such that 
    $\forall \position_1,\position_2,\signal_1,\signal_2$    $f(\position_1,\position_2,\signal_1,\signal_2)=g(\left|\position_2-\position_1\right|,\signal_1,\signal_2)$
    
Variogram:
\begin{equation}
    G(h)=E[(\Signal(\position+h)-\Signal(\position))((\Signal(\position+h)-\Signal(\position)))^{T}]
\end{equation}


Variogram:
\begin{equation}
    G(h)=E[(Y(\mathbf{x}+h)-Y(\mathbf{x}))((Y(\mathbf{x}+h)-Y(\mathbf{x})))^{T}]
\end{equation}

{\color{red}Define the sample here, define $I_{\position_1,\position_2}$}

Sample variogram:
\begin{equation} \label{somelabel}
    \hat{G}(h)=\sum_{ij\in\Sample}{\beta_{ij}(h)(Y(\mathbf{x}_{i})-Y(\mathbf{x}_{j}))(Y(\mathbf{x}_{i})-Y(\mathbf{x}_{j}))^{T}}
\end{equation}
In equation , the term $\beta_{i,j}$ is a mapping that may depend on the sample selected.
Below we give possible definitions of $\beta_{i,j}$:
\begin{equation}
    \beta(\position_1,\position_2,h)=
    \left|\begin{array}{l}
    1 \text{ if } |\mathbf{x}_1-\mathbf{x}_2|=h, 0 \text{ otherwise}\\
    \paramnuisance(\frac{|\mathbf{x}_{i}-\mathbf{x}_{j}|-h}{\alpha})
    \end{array}\right.
\end{equation},

For binned estimation, a finite partition $(B_1,\ldots,B_n)$ of $\mathbb{R}^+$ is given as well as an element $h_k$ of each element $B_k$ of the partition. 
Then the covariogram is first estimated for each $h_k$, as 
...
Then a covariofram is fitted 
...

In the following, we will not consider binned estimation.


where {\color{red} $\paramnuisance$ is ..., and the bins $\mathrm{bin}_h=\left\{(i,j)\in\Sample^2\mid \left|\position_i-\position_j\right|\in [a_h,b_h)\right\}$}

\begin{equation}
    E[\hat{G}(h)]=\sum_{i,j\in\Sampleindex}E\left[ \beta(\position_{(i)},\position_{(j)},h)(Y(\mathbf{x}_{(i)})-Y(\mathbf{x}_{(j)}))(Y(\mathbf{x}_{(i)})-Y(\mathbf{x}_{(j)}))^{\mathrm{T}}\right]
\end{equation}


In the case where $\beta_{\position_i,\position_j,h}$ is not random,

\begin{equation}
    E[\hat{G}(h)]=\sum_{\position_1,\position_2\in \Pop}{\beta()E\left[E\left[(Y(\mathbf{x}_{i})-Y(\mathbf{x}_{j}))^{2}|I\right]I(\position_1)I(\position_2)\right]}
\end{equation}

it is also equal to 


\begin{equation}
    E[\hat{G}(h)]=\sum_{\position_1,\position_2\in \Pop}{\beta()E\left[E\left[(Y(\mathbf{x}_{i})-Y(\mathbf{x}_{j}))^{2}|I(\position_1),I(\position_2)\right]I(\position_1)I(\position_2)\right]}
\end{equation}


Under certain conditions on the sample, for $i:\Pop\to\{0,1\}$, do we have 

\begin{eqnarray*}
\lefteqn{E\left[(Y(\mathbf{x}_{1})-Y(\mathbf{x}_{2}))(Y(\mathbf{x}_{1})-Y(\mathbf{x}_{2}))^T|I=i\right]}\\
&=&
E\left[(Y(\mathbf{x}_{1})-Y(\mathbf{x}_{2}))(Y(\mathbf{x}_{1})-Y(\mathbf{x}_{2}))^T|(I(\position_1),I(\position_2))=(i(\position_1),i(\position_2))\right]?\end{eqnarray*}

Let figure it out another day.


\begin{equation}
    E[\hat{G}(h)]=\sum{\beta()E\left[E\left[(Y(\mathbf{x}_{i})-Y(\mathbf{x}_{j}))^{2}|I\right]I\right]}
\end{equation}

\begin{equation}
\begin{split}
    \density_{(Y(\mathbf{x}_{1}),Y(\mathbf{x}_{2})|I(\position_1)I(\position_2)=1)}\left(\signal_1,\signal_2\right)&=\frac{\density_{Y(\mathbf{x}_{i}),Y(\mathbf{x}_{j})I(\position_1)I(\position_2)}(\signal_1,\signal_2,1,1)}{\int{f_{Y(\mathbf{x}_{i}),Y(\mathbf{x}_{j})I_{ij}}(y_{1},y_{2},1)}d y_{1}d y_{2}}\\
    &=\frac{\density_{Y(\mathbf{x}_{i}),Y(\mathbf{x}_{j})(\cdot,\cdot)}\cdot \density_{I(\position_1),I(\position_1)|(Y(\mathbf{x}_{1}),Y(\mathbf{x}_{2}))=(\signal_1,\signal(2))}(1,1)}{\int{f_{Y(\mathbf{x}_{i}),Y(\mathbf{x}_{j})I_{ij}(y_{1},y_{2},1)}d y_{1}d y_{2}}}\\
    &=\frac{\density_{Y(\mathbf{x}_{i}),Y(\mathbf{x}_{j})(\cdot,\cdot)}\cdot P(I(\position_1)I(\position_2)=1|(Y(\mathbf{x}_{1}),Y(\mathbf{x}_{2}))=(\signal_1,\signal_2)}{P(I(\position_1)I(\position_2)=1)}\\
    &=\rho(\signal_1,\signal_2)~\density_{Y(\mathbf{x}_{i}),Y(\mathbf{x}_{j})}(\signal_1,\signal_2) 
\end{split}
\end{equation}
\begin{equation}
    E[(Y(\mathbf{x}_{i})-Y(\mathbf{x}_{j}))^2|I_{ij}=1]
\end{equation}
Sample variogram: Property, under assumption (g assumption ...)
\begin{equation}
    \begin{split}
        \int{(Y(\mathbf{x}_{i})-Y(\mathbf{x}_{j}))^2dP^{Y(\mathbf{x}_{1}),Y(\mathbf{x}_{j})|I_{ij}=1}}=\int{\left(\signal_2-\signal_1\right)\left(\signal_2-\signal_1\right)^{\mathrm{T}}g(h,y_{1},y_{2})\rho(h,y_{1},y_{2})d y_{1} d y_{2}}
    \end{split}
\end{equation}

Sample distribution, for a given i

$$P^{(\Signal(\position))_{i(\position)=1}\mid I=i}$$


In order to understand the concept of informativeness, we need to distinguish between the population distribution and the sample distribution. In fact, as first definition, we say that a sample is informative if the distribution of the $y$ in the sample is different from the distribution of the $y$ in the population. In more detail, following \cite{pfefferman_1992}, the model holding on the sample can be different from the model holding in the population, due to selection mechanism. In particular, the joint pdf of all the available data is given by

\begin{equation} \label{eq:all_joint}
    f\left(\Signal_{s},\indicator,\Desvar;\theta,\paramnuisance,\rho\right)=\int{f\left(\Signal_{s},\Signal_{\bar{s}}|\Desvar;\theta_{1}\right)P\left(\indicator|\Signal,\Desvar;\rho_{1}\right)g\left(\Desvar;\paramnuisance\right)d\Signal_{\bar{s}}}
\end{equation}
while the pdf that does not take into account the selection mechanism is given by
\begin{equation} \label{eq:joint}
    f\left(\Signal_{s},\Desvar;\theta,\paramnuisance\right)=\int{f\left(\Signal_{s},\Signal_{\bar{s}}|\Desvar;\theta_{1}\right)g\left(\Desvar;\paramnuisance\right)d\Signal_{\bar{s}}}
\end{equation}
If the inference performed by \eqref{eq:all_joint} is equivalent to the inference performed by \eqref{eq:joint}, then the mechanism selection is said to be ignorable. A the contrary, we have an informative sample and we should take into consideration the selection mechanism in order to avoid the introduction of eventual bias in the final estimates.


{\color{red} delete this:\subsection{Sample Distribution of $\Signal$}




Conditionally on y, draw 1000 samples.
do non parametric logistic regression,
kernel: $E[I\mid Y]$ gives you an estimate of $\rho$.
Compute non parametric kernel regression estimator of $E[Z\mid Y=y]$.


\section{?}
\begin{eqnarray*}
\lefteqn{\Cov\left[\Desvar(\position'_1),\Desvar(\position'_2)\mid \left.\Signal\right|_{\{\position_{1},\dots,\position_{m}\}}=\signal
\right]}\\&=&2\provar^2_\varepsilon \left(\Semivariogram_\varepsilon(0)+\Semivariogram_\varepsilon(\position_2'-\position_1')\right)+\beta^2(
\provar_{\position'_1,\position'_2}-\provar_{\position'_1,\position}\provar_{\position,\position}^{-1}\provar_{\position,\position'_2})\\
&=& 2\sigma^2_\varepsilon \left(1+\Semivariogram_\varepsilon(\position_2'-\position_1')\right)+\beta^2(
(2(1+\Semivariogram_\Signal(\position'_2-\position'_1)-\provar_{\position'_1,\position}\provar_{\position,\position}^{-1}\provar_{\position,\position'_2})\\
&=& 2\left(\sigma^2_\varepsilon \left(1+\Semivariogram_\varepsilon(\position_2'-\position_1')\right)+\beta^2
\sigma^2_Y\left(\left(1-\Semivariogram_\Signal(\position'_2-\position'_1)\right)-\sum_{i,j=1}^n \left(1-\Semivariogram(\position'_1-\position_i\right)\left(1-\Semivariogram(\position'_2-\position_j)\right)B_{i,j}\right)\right)
\end{eqnarray*},

}



\begin{property}\label{Prop:1}
If 
[exchangeability condition on $\sampledensity_{\Sampleindex\mid \Desvar}(\position\mid \desvar)$] and 
[exchangeability condition on $P^Z$] 
then $\density_{\Sample[\Sampleindex]}(\position)=\dominantU(\Pop)^{-\#\Sampleindex}$.
\end{property}
\begin{proof}
Let show that $\density_{\Sample[\Sampleindex]}(\position)=\dominantU(\Pop)^{-\#\Sampleindex}$.
\begin{itemize}
\item[]
Let $\Sampleindex\subset\{1,\ldots,n\}$.
By definition $\density_{\Sample[\Sampleindex]}$ is a density on $\Pop^\Sampleindex$, so
$\int_{\Pop^\Sampleindex}\density_{\Sample[\Sampleindex]}(\position)\derive\dominantU^{\otimes\Sampleindex}(\position)=1$.
\item[]
Let show that $\density_{\Sample[\Sampleindex]}(\position)$ is constant.
\begin{itemize}
\item[]
Let assume that for any permutation $\permutation$ of

Let $\permutation$ be a permutation of $\{1,\ldots,N\}$. 
then $\sampledensity_{\Sampleindex\mid \Desvar}(\sigma.\position)=\int_{\DesvarSpace}\sampledensity_{\Sampleindex\mid \Desvar}(\sigma.\position\mid\desvar)\derive P^\Desvar(\desvar) $
\end{itemize}
\end{itemize}
Exchangeability with respect to $\dominantU$ of $....$ and $....$  implies the uniformity of the distribution of $\Sample$.
\end{proof}
When the conditions of Property \ref{Prop:1} hold, we have


\begin{equation}
\rho_{\Sampleindex}\left(\position \mid \signal\right)=
    \frac{\sampledensity_{\Sampleindex\mid \Signal[\Sample[\Sampleindex]]}\left(\position|\signal\right)}
    {\dominantU(\Pop)^{\#\Sampleindex}}.
\end{equation}



\end{document}


{\color{red}
$A=\provar_{\position,\position}$ is a symetric positive matrix:
$A=2\sigma^2_Y (\mathrm{I}-[\Semivariogram(|\position_j-\position_i|)]_{i,j})$
There exists $P$ orthogonal, $\Delta$ diagonal such that $A=^tP\Delta P$

Let $B$ be the inverse of $A$
}


